# ğŸƒ Reinforcement Learning â€” Blackjack

> **SOW-BKI258 Reinforcement Learning** Â· Radboud University Â· 2025â€“2026

A comparative study of tabular Reinforcement Learning algorithms applied to the **Blackjack** environment from [Gymnasium](https://gymnasium.farama.org/). We implement and evaluate Dynamic Programming, Monte Carlo, and Temporal Difference methods, and analyse their performance in a written report inside the Jupyter Notebook.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ notebook.ipynb       # Main Jupyter Notebook (environment + report)
â”œâ”€â”€ dp.py                # Dynamic Programming algorithms
â”œâ”€â”€ mc.py                # Monte Carlo algorithms
â”œâ”€â”€ td.py                # Temporal Difference algorithms
â””â”€â”€ requirements.txt     # Project dependencies
```

---

## ğŸ° Environment

We use the **Blackjack-v1** environment from Gymnasium's *Toy Text* collection â€” a classic episodic card game with a discrete state and action space, well-suited for tabular RL methods.

```python
import gymnasium as gym
env = gym.make("Blackjack-v1")
obs, info = env.reset()
```

- **State space:** (player sum, dealer showing card, usable ace)
- **Action space:** `0` = stick, `1` = hit
- **Goal:** Reach a hand value closer to 21 than the dealer without going bust

---

## ğŸ¤– Implemented Algorithms

### Dynamic Programming (`dp.py`)
Requires full knowledge of the environment's transition dynamics.

| Algorithm | Method |
|---|---|
| Policy Evaluation | Iterative, computes state values V(s) |
| Policy Improvement | Greedy policy update |
| Value Iteration | Combined evaluation + improvement |

### Monte Carlo (`mc.py`)
Learns from complete episodes without a model of the environment.

| Algorithm | Method |
|---|---|
| MC Prediction | First-visit, evaluates action values Q(s,a) |
| MC Control | Exploring Starts *or* Îµ-Greedy strategy |

### Temporal Difference (`td.py`)
Bootstraps from incomplete episodes, learning at every step.

| Algorithm | Method |
|---|---|
| TD(0) | One-step value prediction |
| SARSA | On-policy Îµ-Greedy control |
| Q-Learning | Off-policy Îµ-Greedy control |

> A **random baseline agent** is also included for benchmarking.

---

## ğŸ“Š Report

The report is written **inside** `notebook.ipynb` and covers:

- **Introduction** â€” environment description, agent objective, research question
- **Dynamic Programming** â€” algorithm descriptions, parameter sensitivity (Î³, Î¸), policy plots
- **Monte Carlo** â€” prediction and control results, hyperparameter tuning
- **Temporal Difference** â€” SARSA vs Q-Learning comparison, Îµ decay strategy
- **Comparison & Discussion** â€” MC vs TD metrics (cumulative reward, RMSE, sample efficiency)
- **Conclusion** â€” key findings and algorithm trade-offs

*Target length: ~1000â€“1500 words.*

---

## âš™ï¸ Setup

```bash
# Clone the repository
git clone <repo-url>
cd <repo-folder>

# Install dependencies
pip install -r requirements.txt
```

**`requirements.txt`** includes at minimum:
```
gymnasium
numpy
matplotlib
jupyter
```

---

## ğŸ“… Deadlines

| Date | Milestone |
|---|---|
| 09 Feb 2026 | Workgroup 1 â€” Environment setup |
| 20 Feb 2026 | Workgroup 2 â€” Dynamic Programming |
| 23 Feb 2026 | Workgroup 3 â€” Monte Carlo |
| 02 Mar 2026 | Workgroup 4 â€” Temporal Difference |
| 09 Mar 2026 | Workgroup 5 â€” Overflow / comparison |
| **05 Apr 2026** | **Final submission deadline (23:59)** |

---

## ğŸ“‹ Grading Overview

| Component | Points |
|---|---|
| Dynamic Programming (code + report) | 2 pt |
| Monte Carlo (code + report) | 2 pt |
| Temporal Difference (code + report) | 2 pt |
| Report (intro, results, discussion, style) | 2 pt |
| Bonus (deep RL, exceptional environment) | +1 pt |

> Note: Predefined Gymnasium environments are not eligible for environment originality/correctness points. Max grade is capped at 10.0.

---

## ğŸ‘¥ Group

Made with â™ ï¸ by Group **[X]** â€” Radboud University, Period 3, 2025â€“2026